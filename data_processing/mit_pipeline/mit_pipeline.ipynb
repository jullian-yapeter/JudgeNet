{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakeisrael/USC/Spring 2023/Multimodan Human Communication/project/JudgeNet/.env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from judgenet.modules.preprocess import SentenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1378, 21)\n",
      "(138, 2)\n",
      "(690, 59)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../../data/mit_interview\"\n",
    "\n",
    "scores = pd.read_csv(f\"{folder_path}/turker_scores_full_interview.csv\")\n",
    "transcripts = pd.read_csv(f\"{folder_path}/interview_transcripts_by_turkers.csv\")\n",
    "prosody_features = pd.read_csv(f\"{folder_path}/prosodic_features.csv\")\n",
    "\n",
    "print(scores.shape)\n",
    "print(transcripts.shape)\n",
    "print(prosody_features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract aggregated scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Participant   Overall\n",
      "9             p1  5.297316\n",
      "19            p3  4.414892\n",
      "29            p4  4.494494\n",
      "39            p5  5.457670\n",
      "49            p6  5.106512\n",
      "...          ...       ...\n",
      "1338        pp83  6.045748\n",
      "1348        pp84  5.710073\n",
      "1358        pp85  5.626074\n",
      "1368        pp86  4.853881\n",
      "1377        pp89  4.960084\n",
      "\n",
      "[138 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reduce scores to only Overall score aggregated across turkers\n",
    "scores = scores[scores[\"Worker\"]==\"AGGR\"][[\"Participant\", \"Overall\"]]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_tensor = torch.tensor(scores[\"Overall\"].values)\n",
    "scores_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate VGGish features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"../../data/mit_interview/features/vggish/\"\n",
    "filenames = os.listdir(root)\n",
    "feats = []\n",
    "filenames_ordered = []\n",
    "\n",
    "\n",
    "for i in range(90):\n",
    "    file = f\"audio_P{i}.pt\"\n",
    "    if file in filenames:\n",
    "        filenames_ordered.append(file)\n",
    "\n",
    "for i in range(90):\n",
    "    file = f\"audio_PP{i}.pt\"\n",
    "    if file in filenames:\n",
    "        filenames_ordered.append(file)\n",
    "\n",
    "for name in filenames_ordered:\n",
    "    feats.append(torch.load(f\"{root}/{name}\"))\n",
    "audio_tensor = torch.stack(feats, dim=0)\n",
    "torch.save(audio_tensor, f\"{folder_path}/features/audio.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../data/mit_interview/features/lexical_batched\"\n",
    "feats = []\n",
    "filenames_ordered = []\n",
    "\n",
    "for i in range(138):\n",
    "    file = f\"{i}.pt\"\n",
    "    feats.append(torch.load(f\"{root}/{file}\"))\n",
    "\n",
    "lexical_tensor = torch.stack(feats, dim=0)\n",
    "torch.save(lexical_tensor, f\"{folder_path}/features/lexical.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and pool prosody features (deprecated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through prosody features, taking the mean of each participants' 5 questions\n",
    "# pooled_example = np.mean(prosody_features.iloc[0:5])\n",
    "\n",
    "prosodic_features_pooled = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(0,len(prosody_features),5):\n",
    "    # Check first and last id to make sure chunk is all from same participant\n",
    "    participant = prosody_features.iloc[i][\"participant&question\"].split('Q')[0]\n",
    "    if participant != prosody_features.iloc[4+i][\"participant&question\"].split('Q')[0]:\n",
    "        print(\"Misaligned chunk: \", i)\n",
    "        break\n",
    "\n",
    "    chunk = prosody_features.iloc[i:i+5, prosody_features.columns != \"participant&question\"]\n",
    "\n",
    "    pooled = np.mean(chunk)\n",
    "    # Add participant to row\n",
    "    # pooled = pd.concat([pd.Series([participant.lower()], index=[\"Participant\"]), pooled])\n",
    "\n",
    "    prosodic_features_pooled = pd.concat([prosodic_features_pooled, pooled], axis=1)\n",
    "\n",
    "prosodic_features_pooled = prosodic_features_pooled.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosodic_features_pooled)\n",
    "prosodic_features_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138, 57])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prosody_tensor = torch.tensor(prosodic_features_pooled.values)\n",
    "prosody_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune Transcript down to just interviewee speech and extract BERT features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Kernel was crashing when extracting all the rows, so I'm batching it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def extract_bert(transcript):\n",
    "    split = transcript.split('Interviewee:')\n",
    "\n",
    "    # Trim transcript to Interviewee speech\n",
    "    cleaned = \"\"\n",
    "    for row in split[1:]:\n",
    "        cleaned += row.split('|')[0]\n",
    "\n",
    "    tokens = se.tokenize(cleaned)\n",
    "    return se.encode_batched_tokens(tokens)\n",
    "\n",
    "se = SentenceEncoder()\n",
    "current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished up to row row 139\n"
     ]
    }
   ],
   "source": [
    "lexical_features = []\n",
    "\n",
    "for i in range(current_pos, current_pos+20):\n",
    "    if i < len(transcripts):\n",
    "        transcript = transcripts.iloc[i][\"Transcript\"]\n",
    "        embedding = extract_bert(transcript)\n",
    "        lexical_features.append(embedding)\n",
    "\n",
    "current_pos = current_pos + 20\n",
    "print(f\"finished up to row row {i}\")\n",
    "stacked = torch.stack(lexical_features)\n",
    "torch.save(stacked, f\"{folder_path}/features/lexical_batched/{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "filenames = [\"19.pt\", \"39.pt\", \"59.pt\", \"79.pt\", \"99.pt\", \"119.pt\", \"139.pt\"]\n",
    "for name in filenames:\n",
    "    feats.append(torch.load(f\"{folder_path}/features/lexical/{name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_tensor = torch.cat(feats, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audio: torch.Size([138, 128])\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Shape of scores: {scores_tensor.shape}\")\n",
    "# print(f\"Shape of prosody: {prosody_tensor.shape}\")\n",
    "print(f\"Shape of audio: {audio_tensor.shape}\")\n",
    "# print(f\"Shape of lexical: {lexical_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scores_tensor, f\"{folder_path}/features/scores.pt\")\n",
    "torch.save(prosody_tensor, f\"{folder_path}/features/prosody.pt\")\n",
    "torch.save(lexical_tensor, f\"{folder_path}/features/lexical.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch under here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([138]) torch.Size([138, 128]) torch.Size([138, 768])\n"
     ]
    }
   ],
   "source": [
    "scores_tensor = torch.load(f\"{folder_path}/features/scores.pt\")\n",
    "audio_tensor = torch.load(f\"{folder_path}/features/audio.pt\")\n",
    "lexical_tensor = torch.load(f\"{folder_path}/features/lexical.pt\")\n",
    "\n",
    "print(scores_tensor.shape, audio_tensor.shape, lexical_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
